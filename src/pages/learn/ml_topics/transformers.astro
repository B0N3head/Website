---
import "@/styles/stylesheets/global.css"
import "@/styles/stylesheets/ml-topics.css"
import Navigation from "@/components/Navigation.astro"
import Footer from '@/components/Footer.astro';
---

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width" />
    <title>QUT AI & ML Society</title>
  </head>
  <body class="background-network">
    <Navigation />

    <main class="main-content">
      <div class="page-header">
        <h1 class="page-title">Transformers</h1>
      </div>

      <div class="content-wrapper">

        <!-- Step-by-Step Tutorials -->
        <h2>Step-by-Step Tutorials</h2>
        <div class="tutorial-grid">

          <a
            href="https://markaicode.com/getting-started-transformers-tutorial/"
            target="_blank"
            class="tutorial-box"
          >
            <h3>Getting Started with Transformers: Your First 10 Minutes</h3>
            <p>
              Build your first Transformer model in Python using Hugging Face’s
              pipeline—tokenize, infer, and explore outputs with minimal code.
            </p>
            <span class="tutorial-arrow">→</span>
          </a>

          <a
            href="https://discuss.huggingface.co/t/tutorial-implementing-transformer-from-scratch-a-step-by-step-guide/132158"
            target="_blank"
            class="tutorial-box"
          >
            <h3>Implementing Transformer from Scratch</h3>
            <p>
              Hands-on guide to coding positional embeddings, multi-head
              attention, encoder/decoder layers, and training loop in Python.
            </p>
            <span class="tutorial-arrow">→</span>
          </a>

          <a
            href="https://pylessons.com/transformers-introduction"
            target="_blank"
            class="tutorial-box"
          >
            <h3>Introduction to Transformers – PyLessons</h3>
            <p>
              Learn how to implement embedding layers, self-attention, and
              build a basic Transformer model in TensorFlow step by step.
            </p>
            <span class="tutorial-arrow">→</span>
          </a>

        </div>

        <hr class="divider" />

        <!-- Video Tutorials -->
        <section class="video-section">
          <h2>Video Tutorials</h2>
          <div class="tutorial-grid">

            <div class="tutorial-box">
              <div class="video-wrapper">
                <iframe
                  src="https://www.youtube.com/embed/gm8DUJJhmY4"
                  title="Hugging Face Transformers in 40 Lines of Code"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                  allowfullscreen>
                </iframe>
              </div>
              <h3>Hugging Face Transformers in 40 Lines of Code</h3>
              <p>
                Live coding demo: load a pretrained model, tokenize text, and
                run inference in under 40 lines with Hugging Face.
              </p>
            </div>

            <div class="tutorial-box">
              <div class="video-wrapper">
                <iframe
                  src="https://www.youtube.com/embed/M6uPTYuhdjo"
                  title="Transformer From Scratch in Python (PyTorch)"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                  allowfullscreen>
                </iframe>
              </div>
              <h3>Transformer From Scratch in Python (PyTorch)</h3>
              <p>
                Step-by-step PyTorch implementation: positional encoding,
                multi-head attention, encoder blocks and training loop.
              </p>
            </div>

            <div class="tutorial-box">
              <div class="video-wrapper">
                <iframe
                  src="https://www.youtube.com/embed/8CIjLwZIxO8"
                  title="Fine-tune BERT for Text Classification with Hugging Face"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                  allowfullscreen>
                </iframe>
              </div>
              <h3>Fine-tune BERT for Text Classification</h3>
              <p>
                Hands-on guide: load a pre-trained BERT model, fine-tune on your
                dataset, and evaluate performance using Hugging Face Transformers.
              </p>
            </div>

          </div>
        </section>

      </div>
    </main>

    <Footer />
  </body>
</html>
